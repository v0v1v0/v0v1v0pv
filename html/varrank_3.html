<div class="container">

<table style="width: 100%;"><tr>
<td>mi.data</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Empirical Estimate of the Mutual Information from a Table of Counts</h2>

<h3>Description</h3>

<p>This function estimates the mutual information from observed data</p>


<h3>Usage</h3>

<pre><code class="language-R">mi.data(X, Y, discretization.method=NULL, k=NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>a data frame containing only numeric or continuous variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>a data frame containing only numeric or continuous variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>discretization.method</code></td>
<td>
<p>a character vector giving the discretization method to use. See <code>discretization</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>in case of purely continuous dataset, the mutual information can be computed using the k-nearest neighbours.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The mutual information estimation is computed from the observed frequencies through a plugin estimator based on entropy or using the estimator described in A. Kraskov, H. Stogbauer and P.Grassberger (2004) when the data frame is exclusively made of continuous variables.
</p>
<p>The plugin estimator is I(X, Y) = H (X) + H(Y) - H(X, Y), where H() is the entropy computed with <code>entropy.data</code>.</p>


<h3>Value</h3>

<p>Mutual information estimate.</p>


<h3>Author(s)</h3>

<p>Gilles Kratzer</p>


<h3>References</h3>

<p>Kraskov, A., Stogbauer, H. and  Grassberger, P. (2004) Estimating mutual information. <em>Physical Review E</em>, 69:066138, 1â€“16.</p>


<h3>Examples</h3>

<pre><code class="language-R">Y &lt;- rnorm(n = 100, mean = 0, sd = 2)
X &lt;- rnorm(n = 100, mean = 5, sd = 2)

mi.data(X = Y, Y = X, discretization.method = "sturges")
</code></pre>


</div>