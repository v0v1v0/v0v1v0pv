<div class="container">

<table style="width: 100%;"><tr>
<td>vi_shap</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>SHAP-based variable importance</h2>

<h3>Description</h3>

<p>Compute SHAP-based VI scores for the predictors in a model. See details
below.
</p>


<h3>Usage</h3>

<pre><code class="language-R">vi_shap(object, ...)

## Default S3 method:
vi_shap(object, feature_names = NULL, train = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>A fitted model object (e.g., a
randomForest object).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments to be passed on to <code>fastshap::explain()</code>
(e.g., <code>nsim =  30</code>, <code>adjust = TRUE</code>, or avprediction wrapper via the
<code>pred_wrapper</code> argument); see <code>?fastshap::explain</code> for details on these and
other useful arguments.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>feature_names</code></td>
<td>
<p>Character string giving the names of the predictor
variables (i.e., features) of interest. If <code>NULL</code> (the default) then they
will be inferred from the <code>train</code> and <code>target</code> arguments (see below). It is
good practice to always specify this argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train</code></td>
<td>
<p>A matrix-like R object (e.g., a data frame or matrix)
containing the training data. If <code>NULL</code> (the default) then the
internal <code>get_training_data()</code> function will be called to try and extract it
automatically. It is good practice to always specify this argument.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This approach to computing VI scores is based on the mean absolute
value of the SHAP values for each feature; see, for example,
<a href="https://github.com/shap/shap">https://github.com/shap/shap</a> and the references therein.
</p>
<p>Strumbelj, E., and Kononenko, I. Explaining prediction models and individual
predictions with feature contributions. Knowledge and information systems
41.3 (2014): 647-665.
</p>


<h3>Value</h3>

<p>A tidy data frame (i.e., a tibble object) with two
columns:
</p>

<ul>
<li> <p><code>Variable</code> - the corresponding feature name;
</p>
</li>
<li> <p><code>Importance</code> - the associated importance, computed as the mean absolute
Shapley value.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
library(ggplot2)  # for theme_light() function
library(xgboost)

# Simulate training data
trn &lt;- gen_friedman(500, sigma = 1, seed = 101)  # ?vip::gen_friedman

# Feature matrix
X &lt;- data.matrix(subset(trn, select = -y))  # matrix of feature values

# Fit an XGBoost model; hyperparameters were tuned using 5-fold CV
set.seed(859)  # for reproducibility
bst &lt;- xgboost(X, label = trn$y, nrounds = 338, max_depth = 3, eta = 0.1,
               verbose = 0)

# Construct VIP using "exact" SHAP values from XGBoost's internal Tree SHAP
# functionality
vip(bst, method = "shap", train = X, exact = TRUE, include_type = TRUE,
    geom = "point", horizontal = FALSE,
    aesthetics = list(color = "forestgreen", shape = 17, size = 5)) +
  theme_light()

# Use Monte-Carlo approach, which works for any model; requires prediction
# wrapper
pfun_prob &lt;- function(object, newdata) {  # prediction wrapper
  # For Shapley explanations, this should ALWAYS return a numeric vector
  predict(object, newdata = newdata, type = "prob")[, "yes"]
}

# Compute Shapley-based VI scores
set.seed(853)  # for reproducibility
vi_shap(rfo, train = subset(t1, select = -survived), pred_wrapper = pfun_prob,
        nsim = 30)
## # A tibble: 5 Ã— 2
## Variable Importance
##   &lt;chr&gt;         &lt;dbl&gt;
## 1 pclass       0.104
## 2 age          0.0649
## 3 sex          0.272
## 4 sibsp        0.0260
## 5 parch        0.0291

## End(Not run)
</code></pre>


</div>