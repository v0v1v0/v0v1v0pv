<div class="container">

<table style="width: 100%;"><tr>
<td>entropy.estimate</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Vasicek estimate of differential Shannon Entropy</h2>

<h3>Description</h3>

<p>Computes Vasicek estimate of differential Shannon entropy from a numeric sample.</p>


<h3>Usage</h3>

<pre><code class="language-R">entropy.estimate(x,window)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>(<code>numeric, vector</code>) the numeric sample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>window</code></td>
<td>
<p>(<code>numeric, single value</code>) an integer between 1 and half on the sample size specifying the window size for computing Vasicek estimate. See Details for additional information.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Vasicek estimator of Shannon entropy is defined, for a random sample <code class="reqn">X_1, \dots, X_n</code>, by
</p>
<p style="text-align: center;"><code class="reqn">\frac{1}{n}\sum_{i=1}^{n} \log (\frac{n}{2m}[X_{(i+m)}-X_{(i-m)}]),</code>
</p>

<p>where <code class="reqn">X_{(i)}</code> is the order statistic, <code class="reqn">m&lt;(n/2)</code> is the window size, and <code class="reqn">X_{(i)}=X_{(1)}</code> for <code class="reqn">i&lt;1</code> and <code class="reqn">X_{(i)}=X_{(n)}</code> for <code class="reqn">i&gt;n</code>. </p>


<h3>Value</h3>

<p> A single numeric value representing the Vasicek estimate of entropy of the sample</p>


<h3>Author(s)</h3>

<p>J. Lequesne <a href="mailto:justine.lequesne@unicaen.fr">justine.lequesne@unicaen.fr</a></p>


<h3>References</h3>

<p>Vasicek, O., A test for normality based on sample entropy, <em>Journal of the Royal Statistical Society,</em> <b>38(1)</b>, 54-59 (1976).</p>


<h3>See Also</h3>

<p><code>vs.test</code> which performs Vasicek-Song goodness-of-fit tests to the specified maximum entropy distribution family.</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(2)
samp &lt;- rnorm(100, mean = 0, s = 1)
entropy.estimate(x = samp, window = 8)
log(2*pi*exp(1))/2 #true value of entropy of normal distribution
</code></pre>


</div>