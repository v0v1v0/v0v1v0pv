<div class="container">

<table style="width: 100%;"><tr>
<td>varSelRF</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Variable selection from random forests using OOB error</h2>

<h3>Description</h3>

<p>Using the OOB error as minimization criterion, carry out variable
elimination from
random forest, by successively eliminating the least important
variables (with importance as returned from random forest).
</p>


<h3>Usage</h3>

<pre><code class="language-R">varSelRF(xdata, Class, c.sd = 1, mtryFactor = 1, ntree = 5000,
         ntreeIterat = 2000, vars.drop.num = NULL, vars.drop.frac = 0.2,
         whole.range = TRUE, recompute.var.imp = FALSE, verbose = FALSE,
         returnFirstForest = TRUE, fitted.rf = NULL, keep.forest = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>xdata</code></td>
<td>
<p>A data frame or matrix, with subjects/cases in rows and
variables in columns. NAs not allowed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Class</code></td>
<td>
<p>The dependent variable; must be a factor.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>c.sd</code></td>
<td>
<p>The factor that multiplies the sd. to decide on stopping
the tierations or choosing the final solution. See reference for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mtryFactor</code></td>
<td>
<p>The multiplication factor of
<code class="reqn">\sqrt{number.of.variables}</code> for the number of variables to use for
the ntry argument of randomForest.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntree</code></td>
<td>
<p>The number of trees to use for the first forest;
same as ntree for randomForest.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntreeIterat</code></td>
<td>
<p>The number of trees to use (ntree of randomForest)
for all additional forests.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vars.drop.num</code></td>
<td>
<p>The number of variables to exclude at each iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vars.drop.frac</code></td>
<td>
<p>The fraction of variables, from those
in the previous forest, to exclude at each iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>whole.range</code></td>
<td>
<p>If TRUE continue dropping variables until a forest
with only two variables is built, and choose the best model from the
complete series of models. If
FALSE, stop the iterations if the current OOB error becomes larger
than the initial OOB error (plus c.sd*OOB standard error) or
if the current OOB error becoems larger than the
previous OOB error (plus c.sd*OOB standard error).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recompute.var.imp</code></td>
<td>
<p>If TRUE recompute variable importances at
each new iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Give more information about what is being done.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>returnFirstForest</code></td>
<td>
<p>If TRUE the random forest from the complete
set of variables is returned.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fitted.rf</code></td>
<td>
<p>An (optional) object of class
randomForest previously fitted. In this case, the ntree and
mtryFactor arguments are obtained from the fitted object, not the
arguments to this function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.forest</code></td>
<td>
<p>Same argument as in randomForest function. If the
forest is kept, it will be returned as part of the "rf.model"
component of the output. Beware that setting this to TRUE can lead
to very large memory consumption.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>With the default parameters, we examine all forest that result from
eliminating, iteratively, a fraction, <code>vars.drop.frac</code>, of the least
important variables used in the previous iteration. By default,
<code>vars.frac.drop = 0.2</code> which allows for relatively fast operation,
is coherent with the idea of an “aggressive variable selection”
approach, and increases the resolution as the number of variables
considered becomes smaller.  By default, we do not recalculate variable
importances at each step (<code>recompute.var.imp = FALSE</code>)
as <cite>Svetnik et al. 2004</cite> mention severe overfitting
resulting from recalculating variable importances. After fitting all
forests, we examine the OOB error rates from all the fitted random
forests. We choose the solution with the smallest number of genes
whose error rate is within <code>c.sd</code> standard errors of the minimum error
rate of all forests. (The standard error is calculated using the 
expression for a biomial error count [<code class="reqn">\sqrt{p (1-p) * 1/N}</code>]).
Setting <code>c.sd = 0</code> is the same as selecting the set of genes that leads
to the smallest error rate.  Setting <code>c.sd = 1</code> is similar to the
common “1 s.e.  rule”, used in the classification trees literature;
this strategy can lead to solutions with
fewer genes than selecting the solution with the smallest error rate,
while achieving an error rate that is not different, within sampling
error, from the “best solution”.
</p>
<p>The use of <code>ntree = 5000</code> and <code>ntreeIterat = 2000</code> is
discussed in longer detail in the references. Essentially, more
iterations rarely seem to lead (with 9 different microarray data sets)
to improved solutions.
</p>
<p>The measure of variable importance used is based on the decrease
of classification accuracy when values of a variable in a node of a
tree are permuted randomly (see references); we use the unscaled
version —see our paper and supplementary material.
</p>


<h3>Value</h3>

<p>An object of class "varSelRF": a list with components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>selec.history</code></td>
<td>
<p>A data frame where the selection history is
stored. The components are:
</p>

<dl>
<dt>Number.Variables</dt>
<dd>
<p>The number of variables examined.</p>
</dd>
<dt>Vars.in.Forest</dt>
<dd>
<p>The actual variables that were in the forest
at that stage.</p>
</dd>
<dt>OOB</dt>
<dd>
<p>Out of bag error rate.</p>
</dd>
<dt>sd.OOB</dt>
<dd>
<p>Standard deviation of the error rate.</p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rf.model</code></td>
<td>
<p>The final, selected, random forest (only if
<code>whole.range = FALSE</code>). (If you set whole.range = TRUE, the
final model always contains exactly two variables. This is unlikely
to be the forest that interests you).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>selected.vars</code></td>
<td>
<p>The variables finally selected.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>selected.model</code></td>
<td>
<p>Same as above, but ordered alphabetically and
concatenated with a "+" for easier display.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>best.model.nvars</code></td>
<td>
<p>The number of variables in the finally
selected model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initialImportance</code></td>
<td>
<p>The importances of variables, before any
variable deletion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initialOrderedImportances</code></td>
<td>
<p>Same as above but ordered in by
decreasing importance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntree</code></td>
<td>
<p>The <code>ntree</code> argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntreeIterat</code></td>
<td>
<p>The <code>ntreeIterat</code> argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mtryFactor</code></td>
<td>
<p>The <code>mtryFactor</code> argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>firstForest</code></td>
<td>
<p>The first forest (before any variable selection) fitted.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Ramon Diaz-Uriarte  <a href="mailto:rdiaz02@gmail.com">rdiaz02@gmail.com</a></p>


<h3>References</h3>

<p>Breiman, L. (2001) Random forests.
<em>Machine Learning</em>, <b>45</b>, 5–32.
</p>
<p>Diaz-Uriarte, R. and Alvarez de Andres,
S. (2005) Variable selection from random forests: application to gene
expression
data. Tech. report.
<a href="http://ligarto.org/rdiaz/Papers/rfVS/randomForestVarSel.html">http://ligarto.org/rdiaz/Papers/rfVS/randomForestVarSel.html</a>
</p>
<p>Svetnik, V., Liaw, A. , Tong, C &amp; Wang, T. (2004) Application of
Breiman's random forest to modeling structure-activity relationships of
pharmaceutical molecules.  Pp. 334-343 in <em>F. Roli, J. Kittler, and T. Windeatt</em>
(eds.). <em>Multiple Classier Systems, Fifth International Workshop</em>, MCS
2004, Proceedings, 9-11 June 2004, Cagliari, Italy. Lecture Notes in
Computer Science, vol. 3077.  Berlin: Springer.
</p>


<h3>See Also</h3>

<p><code>randomForest</code>,
<code>plot.varSelRF</code>,
<code>varSelRFBoot</code></p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(1)
x &lt;- matrix(rnorm(25 * 30), ncol = 30)
colnames(x) &lt;- paste("v", 1:30, sep = "")
x[1:10, 1:2] &lt;- x[1:10, 1:2] + 1
x[1:4, 5] &lt;- x[1:4, 5] - 1.5
x[5:10, 8] &lt;- x[5:10, 8] + 1.4 

cl &lt;- factor(c(rep("A", 10), rep("B", 15)))  
rf.vs1 &lt;- varSelRF(x, cl, ntree = 500, ntreeIterat = 300,
                   vars.drop.frac = 0.2)
rf.vs1
plot(rf.vs1)


## Note you can use tiny vars.drop.frac
## though you'll rarely want this
rf.vs1tiny &lt;- varSelRF(x, cl, ntree = 500, ntreeIterat = 300,
                   vars.drop.frac = 0.01)

#### Using the final, fitted model to predict other data

## Simulate new data
set.seed(2)
x.new &lt;- matrix(rnorm(25 * 30), ncol = 30)
colnames(x.new) &lt;- paste("v", 1:30, sep = "")
x.new[1:10, 1:2] &lt;- x.new[1:10, 1:2] + 1
x.new[1:10, 5] &lt;- x.new[1:10, 5] - 0.5


## Fit with whole.range = FALSE and keep.forest = TRUE
set.seed(3)
rf.vs2 &lt;- varSelRF(x, cl, ntree = 3000, ntreeIterat = 2000,
                   vars.drop.frac = 0.3, whole.range = FALSE,
                   keep.forest = TRUE)


## To obtain predictions from a data set, you must specify the
## same variables as those used in the final model

rf.vs2$selected.vars


predict(rf.vs2$rf.model,
        newdata = subset(x.new, select = rf.vs2$selected.vars))
predict(rf.vs2$rf.model,
        newdata = subset(x.new, select = rf.vs2$selected.vars),
        type = "prob")


## If you had not kept the forest (keep.forest) you could also try
randomForest(y = cl, x = subset(x, select = rf.vs2$selected.vars),
             ntree = rf.vs2$ntreeIterat,
             xtest = subset(x, select = rf.vs2$selected.vars))$test

## but here the forest is built new (with only the selected variables)
## so results need not be the same



## CAUTION: You will NOT want this (these are similar to resubstitution
##   predictions)

predict(rf.vs2$rf.model, newdata = subset(x, select = rf.vs2$selected.vars))

## nor these (read help of predict.randomForest for why these
## predictions are different from those from previous command)

predict(rf.vs2$rf.model)


</code></pre>


</div>