<div class="container">

<table style="width: 100%;"><tr>
<td>negbinomial</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Negative Binomial Distribution Family Function </h2>

<h3>Description</h3>

<p>Maximum likelihood estimation of the two parameters of a negative
binomial distribution.
</p>


<h3>Usage</h3>

<pre><code class="language-R">negbinomial(zero = "size", parallel = FALSE, deviance.arg = FALSE,
            type.fitted = c("mean", "quantiles"),
            percentiles = c(25, 50, 75), vfl = FALSE,
            mds.min = 1e-3, nsimEIM = 500, cutoff.prob = 0.999,
            eps.trig = 1e-7, max.support = 4000, max.chunk.MB = 30,
            lmu = "loglink", lsize = "loglink",
            imethod = 1, imu = NULL, iprobs.y = NULL,
            gprobs.y = ppoints(6), isize = NULL,
            gsize.mux = exp(c(-30, -20, -15, -10, -6:3)))
polya(zero = "size", type.fitted = c("mean", "prob"),
      mds.min = 1e-3, nsimEIM = 500, cutoff.prob = 0.999,
      eps.trig = 1e-7, max.support = 4000, max.chunk.MB = 30,
      lprob = "logitlink", lsize = "loglink", imethod = 1, iprob = NULL,
      iprobs.y = NULL, gprobs.y = ppoints(6), isize = NULL,
      gsize.mux = exp(c(-30, -20, -15, -10, -6:3)), imunb = NULL)
polyaR(zero = "size", type.fitted = c("mean", "prob"),
       mds.min = 1e-3, nsimEIM = 500,  cutoff.prob = 0.999,
       eps.trig = 1e-7, max.support = 4000, max.chunk.MB = 30,
       lsize = "loglink", lprob = "logitlink", imethod = 1, iprob = NULL,
       iprobs.y = NULL, gprobs.y = ppoints(6), isize = NULL,
       gsize.mux = exp(c(-30, -20, -15, -10, -6:3)), imunb = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>zero</code></td>
<td>

<p>Can be an integer-valued vector, and if so, then
it is usually assigned <code class="reqn">-2</code>
or <code class="reqn">2</code>. Specifies which of the two
linear/additive predictors are modelled as an intercept
only. By default, the <code class="reqn">k</code> parameter (after <code>lsize</code>
is applied) is modelled as a single unknown number that
is estimated. It can be modelled as a function of the
explanatory variables by setting <code>zero = NULL</code>; this
has been called a NB-H model by Hilbe (2011). A negative
value means that the value is recycled, so setting <code class="reqn">-2</code>
means all <code class="reqn">k</code> are intercept-only.
See <code>CommonVGAMffArguments</code> for more information.
</p>




</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lmu, lsize, lprob</code></td>
<td>

<p>Link functions applied to the <code class="reqn">\mu</code>, <code class="reqn">k</code>
and <code class="reqn">p</code>  parameters.
See <code>Links</code> for more choices.
Note that the <code class="reqn">\mu</code>, <code class="reqn">k</code>
and <code class="reqn">p</code>  parameters are the <code>mu</code>,
<code>size</code> and <code>prob</code> arguments of
<code>rnbinom</code> respectively.
Common alternatives for <code>lsize</code> are
<code>negloglink</code> and
<code>reciprocallink</code>, and
<code>logloglink</code> (if <code class="reqn">k &gt; 1</code>).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>imu, imunb, isize, iprob</code></td>
<td>

<p>Optional initial values for the mean and <code class="reqn">k</code> and <code class="reqn">p</code>.
For <code class="reqn">k</code>, if failure to converge occurs then try different values
(and/or use <code>imethod</code>).
For a <code class="reqn">S</code>-column response, <code>isize</code> can be of length <code class="reqn">S</code>.
A value <code>NULL</code> means an initial value for each response is
computed internally using a gridsearch based on <code>gsize.mux</code>.
The last argument is ignored if used within <code>cqo</code>; see
the <code>iKvector</code> argument of <code>qrrvglm.control</code> instead.
In the future <code>isize</code> and <code>iprob</code> might be depreciated.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nsimEIM</code></td>
<td>

<p>This argument is used
for computing the diagonal element of the
<em>expected information matrix</em> (EIM) corresponding to <code class="reqn">k</code>
based on the <em>simulated Fisher scoring</em> (SFS) algorithm.
See <code>CommonVGAMffArguments</code> for more information
and the notes below.
SFS is one of two algorithms for computing the EIM elements
(so that both algorithms may be used on a given data set).
SFS is faster than the exact method when <code>Qmax</code> is large.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cutoff.prob</code></td>
<td>

<p>Fed into the <code>p</code> argument
of <code>qnbinom</code>
in order to obtain an upper limit for the approximate
support of the distribution, called <code>Qmax</code>, say.
Similarly, the value <code>1-p</code> is
fed into the <code>p</code> argument
of <code>qnbinom</code>
in order to obtain a lower limit for the approximate
support of the distribution, called <code>Qmin</code>, say.
Hence the approximate support is <code>Qmin:Qmax</code>.
This argument should be
a numeric and close to 1 but never exactly 1.
Used to specify how many terms of the infinite series
for computing the second diagonal element of the
EIM are actually used.
The closer this argument is to 1, the more accurate the
standard errors of the regression coefficients will be.
If this argument is too small, convergence will take longer.
</p>





</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.chunk.MB, max.support</code></td>
<td>

<p><code>max.support</code> is used to describe the eligibility of
individual observations
to have their EIM computed by the <em>exact method</em>.
Here, we are concerned about
computing the EIM wrt <code class="reqn">k</code>.
The exact method algorithm operates separately on each response
variable,
and it constructs a large matrix provided that the number of
columns is less than <code>max.support</code>.
If so, then the computations are done in chunks, so
that no more than about <code>max.chunk.MB</code> megabytes
of memory is used at a time (actually, it is proportional to
this amount).  Regarding eligibility of this algorithm, each
observation must have the length of the vector, starting from
the <code>1-cutoff.prob</code> quantile
and finishing up at the <code>cutoff.prob</code> quantile,
less than <code>max.support</code>
(as its approximate support).
If you have abundant memory then you might try setting
<code>max.chunk.MB = Inf</code>, but then the computations might take
a very long time.
Setting <code>max.chunk.MB = 0</code> or <code>max.support = 0</code>
will force the EIM to be computed using the SFS algorithm only
(this <em>used to be</em> the default method for <em>all</em>
the observations).  When the fitted values of the model are
large and <code class="reqn">k</code> is small, the computation of the EIM will be
costly with respect to time and memory if the exact method is
used. Hence the argument <code>max.support</code> limits the cost in
terms of time.  For intercept-only models <code>max.support</code>
is multiplied by a number (such as 10) because only one inner
product needs be computed.  Note: <code>max.support</code> is an
upper bound and limits the number of terms dictated by the
<code>eps.trig</code> argument.
</p>


</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mds.min</code></td>
<td>

<p>Numeric.
Minimum value of the NBD mean divided by <code>size</code> parameter.
The closer this ratio is to 0, the closer the distribution is
to a Poisson.
Iterations will stop when an estimate of <code class="reqn">k</code> is so large,
relative to the mean, than it is below this threshold
(this is treated as a boundary of the parameter space).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vfl</code></td>
<td>

<p>Logical.
Fit the 
Varianceâ€“variance
Factorized
Loglinear
(VFL)
model?
If <code>TRUE</code> then the constraint matrix
<code>rbind(0, -1)</code> is assigned to all covariates
which are not parallel.
Hence <code>parallel</code> must be used
in conjunction with this argument
to specify the set of covariates used for
modelling the mean.
Note that the constraint matrix for the
intercept should be parallel too.
The overall
resulting parameterization is the same
as Evans (1953).
Some general information is at
<code>CommonVGAMffArguments</code>.
</p>



</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps.trig</code></td>
<td>

<p>Numeric.
A small positive value used in the computation of the EIMs.
It focusses on the denominator of the terms of a series.
Each term in the series (that is used to approximate an infinite
series) has a value greater than <code>size / sqrt(eps.trig)</code>,
thus very small terms are ignored.  It's a good idea to set
a smaller value that will result in more accuracy, but it
will require a greater computing time (when <code class="reqn">k</code> is close
to 0).  And adjustment to <code>max.support</code> may be needed.
In particular, the quantity computed by special means is
<code class="reqn">\psi'(k) - E[\psi'(Y+k)]</code>,
which is the difference between two <code>trigamma</code>.
functions. It is part of the calculation of the EIM with respect
to the <code>size</code> parameter.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gsize.mux</code></td>
<td>

<p>Similar to <code>gsigma</code> in <code>CommonVGAMffArguments</code>.
However, this grid is multiplied by the initial
estimates of the NBD mean parameter.
That is, it is on a relative scale rather than on an
absolute scale.
If the counts are very large in value then convergence fail might
occur; if so, then try a smaller value such as
<code>gsize.mux = exp(-40)</code>.
</p>
</td>
</tr>
</table>
<table>
<tr style="vertical-align: top;">
<td><code>type.fitted, percentiles</code></td>
<td>

<p>See <code>CommonVGAMffArguments</code> for more information.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>deviance.arg</code></td>
<td>

<p>Logical.
If <code>TRUE</code>, the deviance is computed <em>after</em> convergence.
It only works in the NB-2 model.
It is also necessary to set <code>criterion = "coefficients"</code>
or <code>half.step = FALSE</code>
since
one cannot use that criterion properly for the minimization
within the IRLS algorithm.
It should be set <code>TRUE</code> when
used with <code>cqo</code> under the fast algorithm.
</p>








</td>
</tr>
<tr style="vertical-align: top;">
<td><code>imethod</code></td>
<td>

<p>An integer with value <code>1</code> or <code>2</code> etc. which
specifies the initialization method for the <code class="reqn">\mu</code>
parameter.  If failure to converge occurs try another value
and/or else specify a value for <code>iprobs.y</code> and/or else
specify a value for <code>isize</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>

<p>Setting <code>parallel = TRUE</code> is useful in order to get
something similar to <code>quasipoisson</code> or
what is known as NB-1.
If <code>parallel = TRUE</code> then the parallelism constraint
does not apply to any intercept term.
You should set <code>zero = NULL</code> too if <code>parallel =
  TRUE</code> to avoid a conflict.
See <code>CommonVGAMffArguments</code> for more
information.
Argument <code>vfl</code> requires the use of
<code>parallel</code> to fit the VFL model.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gprobs.y</code></td>
<td>

<p>A vector representing a grid;
passed into the <code>probs</code> argument
of <code>quantile</code>
when <code>imethod = 1</code> to obtain an initial value for
the mean of each response. Is overwritten by any value of
<code>iprobs.y</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iprobs.y</code></td>
<td>

<p>Passed into the <code>probs</code> argument
of <code>quantile</code>
when <code>imethod = 1</code> to obtain an initial value for the
mean of each response. Overwrites any value of <code>gprobs.y</code>.
This argument might be deleted in the future.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The negative binomial distribution (NBD)
can be motivated in several ways,
e.g., as a Poisson distribution with a mean that is gamma
distributed.
There are several common parametrizations of the NBD.
The one used by <code>negbinomial()</code> uses the
mean <code class="reqn">\mu</code> and an <em>index</em> parameter
<code class="reqn">k</code>, both which are positive.
Specifically, the density of a random variable <code class="reqn">Y</code> is
</p>
<p style="text-align: center;"><code class="reqn">f(y;\mu,k) = {y + k - 1 \choose y} \,
    \left( \frac{\mu}{\mu+k} \right)^y\,
    \left( \frac{k}{k+\mu} \right)^k </code>
</p>

<p>where <code class="reqn">y=0,1,2,\ldots</code>,
and <code class="reqn">\mu &gt; 0</code> and <code class="reqn">k &gt; 0</code>.
Note that the <em>dispersion</em> parameter is
<code class="reqn">1/k</code>, so that as <code class="reqn">k</code> approaches infinity the
NBD approaches a Poisson distribution.
The response has variance
<code class="reqn">Var(Y)=\mu+\mu^2/k</code>.
When fitted, the <code>fitted.values</code> slot of the object
contains the estimated value of the <code class="reqn">\mu</code> parameter,
i.e., of the mean <code class="reqn">E(Y)</code>.
It is common for some to use <code class="reqn">\alpha=1/k</code> as the
ancillary or heterogeneity parameter;
so common alternatives for <code>lsize</code> are
<code>negloglink</code> and
<code>reciprocallink</code>.
</p>
<p>For <code>polya</code> the density is
</p>
<p style="text-align: center;"><code class="reqn">f(y;p,k) = {y + k - 1 \choose y} \,
    \left( 1 - p \right)^y\,
    p^k </code>
</p>

<p>where <code class="reqn">y=0,1,2,\ldots</code>,
and <code class="reqn">k &gt; 0</code> and <code class="reqn">0 &lt; p &lt; 1</code>.
</p>
<p>Family function <code>polyaR()</code> is the same as <code>polya()</code>
except the order of the two parameters are switched.  The reason
is that <code>polyaR()</code> tries to match with
<code>rnbinom</code> closely
in terms of the argument order, etc.
Should the probability parameter be of primary interest,
probably, users will prefer using  <code>polya()</code> rather than
<code>polyaR()</code>.
Possibly <code>polyaR()</code> will be decommissioned one day.
</p>
<p>The NBD can be coerced into the
classical GLM framework with one of the parameters being
of interest and the other treated as a nuisance/scale
parameter (this is implemented in the <span class="pkg">MASS</span> library). The
<span class="pkg">VGAM</span> family function <code>negbinomial()</code> treats both
parameters on the same footing, and estimates them both
by full maximum likelihood estimation.
</p>


<p>The parameters <code class="reqn">\mu</code> and <code class="reqn">k</code> are independent
(diagonal EIM), and the confidence region for <code class="reqn">k</code>
is extremely skewed so that its standard error is often
of no practical use. The parameter <code class="reqn">1/k</code> has been
used as a measure of aggregation.
For the NB-C the EIM is not diagonal.
</p>
<p>These <span class="pkg">VGAM</span> family functions handle
<em>multiple</em> responses, so that a response matrix can be
inputted. The number of columns is the number
of species, say, and setting <code>zero = -2</code> means that
<em>all</em> species have a <code class="reqn">k</code> equalling a (different)
intercept only.
</p>
<p>Conlisk, et al. (2007) show that fitting the NBD to
presence-absence data will result in identifiability problems.
However, the model is identifiable if the response values include
0, 1 and 2.
</p>

<p>For the NB canonical link (NB-C), its estimation
has a somewhat interesting history.
Some details are at <code>nbcanlink</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"vglmff"</code>
(see <code>vglmff-class</code>).
The object is used by modelling functions such
as <code>vglm</code>,
<code>rrvglm</code>
and <code>vgam</code>.
</p>


<h3>Warning</h3>

<p>Poisson regression corresponds to <code class="reqn">k</code> equalling
infinity.  If the data is Poisson or close to Poisson,
numerical problems may occur.
Some corrective measures are taken, e.g.,
<code class="reqn">k</code> is effectively capped
(relative to the mean) during
estimation to some large value and a warning is issued.
And setting <code>stepsize = 0.5</code> for
half stepping is probably
a good idea too when the data is extreme.
</p>














<p>The NBD is a strictly unimodal distribution. Any data set
that does not exhibit a mode (somewhere in the middle) makes
the estimation problem difficult.  Set <code>trace = TRUE</code>
to monitor convergence.
</p>
<p>These functions are fragile; the maximum likelihood estimate
of the index parameter is fraught (see Lawless, 1987).
Other alternatives to <code>negbinomial</code> are to fit a NB-1 or
RR-NB (aka NB-P) model; see Yee (2014).  Also available are
the NB-C, NB-H and NB-G.  Assigning values to the <code>isize</code>
argument may lead to a local solution, and smaller values are
preferred over large values when using this argument.
</p>

<p>If one wants to force SFS
to be used on all observations, then
set <code>max.support = 0</code> or <code>max.chunk.MB = 0</code>.
If one wants to force the exact method
to be used for all observations, then
set <code>max.support = Inf</code>.
If the computer has <em>much</em> memory, then trying
<code>max.chunk.MB = Inf</code> and
<code>max.support = Inf</code>
may provide a small speed increase.
If SFS is used at all, then the working
weights (<code>@weights</code>) slot of the
fitted object will be a matrix;
otherwise that slot will be a <code>0 x 0</code> matrix.
</p>
<p>An alternative to the NBD is the generalized Poisson
distribution,
<code>genpoisson1</code>,
<code>genpoisson2</code> and
<code>genpoisson0</code>,
since that also handles overdispersion wrt Poisson.
It has one advantage in that its EIM can be computed
straightforwardly.
</p>
<p>Yet to do: write a family function which uses the methods
of moments estimator for <code class="reqn">k</code>.
</p>


<h3>Note</h3>




<p>These 3 functions implement 2 common parameterizations
of the negative binomial (NB). Some people called the
NB with integer <code class="reqn">k</code> the <em>Pascal</em> distribution,
whereas if <code class="reqn">k</code> is real then this is the <em>Polya</em>
distribution. I don't. The one matching the details of
<code>rnbinom</code> in terms of <code class="reqn">p</code>
and <code class="reqn">k</code> is <code>polya()</code>.
</p>
<p>For <code>polya()</code> the code may fail when <code class="reqn">p</code> is close
to 0 or 1. It is not yet compatible with <code>cqo</code>
or <code>cao</code>.
</p>
<p>Suppose the response is called <code>ymat</code>.
For <code>negbinomial()</code>
the diagonal element of the <em>expected information matrix</em>
(EIM) for parameter <code class="reqn">k</code>
involves an infinite series; consequently SFS
(see <code>nsimEIM</code>) is used as the backup algorithm only.
SFS should be better if <code>max(ymat)</code> is large,
e.g., <code>max(ymat) &gt; 1000</code>,
or if there are any outliers in <code>ymat</code>.
The default algorithm involves a finite series approximation
to the support <code>0:Inf</code>;
the arguments
<code>max.memory</code>,
<code>min.size</code> and
<code>cutoff.prob</code> are pertinent.
</p>



<p>Regardless of the algorithm used,
convergence problems may occur, especially when the response
has large outliers or is large in magnitude.
If convergence failure occurs, try using arguments
(in recommended decreasing order)
<code>max.support</code>,
<code>nsimEIM</code>,
<code>cutoff.prob</code>,
<code>iprobs.y</code>,
<code>imethod</code>,
<code>isize</code>,
<code>zero</code>,
<code>max.chunk.MB</code>.
</p>
<p>The function <code>negbinomial</code> can be used by the
fast algorithm in <code>cqo</code>, however, setting
<code>eq.tolerances = TRUE</code> and <code>I.tolerances = FALSE</code>
is recommended.
</p>








<p>In the first example below (Bliss and Fisher, 1953), from each
of 6 McIntosh apple trees in an orchard that had been sprayed,
25 leaves were randomly selected. On each of the leaves,
the number of adult female European red mites were counted.
</p>

<p>There are two special uses of <code>negbinomial</code> for handling
count data.
Firstly,
when used by <code>rrvglm</code>  this
results in a continuum of models in between and
inclusive of quasi-Poisson and negative binomial regression.
This is known as a reduced-rank negative binomial model
<em>(RR-NB)</em>.  It fits a negative binomial log-linear
regression with variance function
<code class="reqn">Var(Y)=\mu+\delta_1 \mu^{\delta_2}</code>
where <code class="reqn">\delta_1</code>
and   <code class="reqn">\delta_2</code>
are parameters to be estimated by MLE.
Confidence intervals are available for <code class="reqn">\delta_2</code>,
therefore it can be decided upon whether the
data are quasi-Poisson or negative binomial, if any.
</p>
<p>Secondly,
the use of <code>negbinomial</code> with <code>parallel = TRUE</code>
inside <code>vglm</code>
can result in a model similar to <code>quasipoisson</code>.
This is named the <em>NB-1</em> model.
The dispersion parameter is estimated by MLE whereas
<code>glm</code> uses the method of moments.
In particular, it fits a negative binomial log-linear regression
with variance function
<code class="reqn">Var(Y) = \phi_0   \mu</code>
where <code class="reqn">\phi_0</code>
is a parameter to be estimated by MLE.
Confidence intervals are available for <code class="reqn">\phi_0</code>.
</p>


<h3>Author(s)</h3>

<p> Thomas W. Yee,
and with a lot of help by Victor Miranda
to get it going with <code>nbcanlink</code>.
</p>


<h3>References</h3>

<p>Bliss, C. and Fisher, R. A. (1953).
Fitting the negative binomial distribution to biological data.
<em>Biometrics</em>
<b>9</b>, 174â€“200.
</p>
<p>Conlisk, E. and Conlisk, J. and Harte, J. (2007).
The impossibility of estimating a negative binomial
clustering parameter from presence-absence data:
A comment on He and Gaston.
<em>The American Naturalist</em>
<b>170</b>,
651â€“654.
</p>

<p>Evans, D. A. (1953).
Experimental evidence concerning contagious
distributions in ecology.
Biometrika, <b>40</b>(1â€“2), 186â€“211.
</p>
<p>Hilbe, J. M. (2011).
<em>Negative Binomial Regression</em>,
2nd Edition.
Cambridge: Cambridge University Press.
</p>
<p>Lawless, J. F. (1987).
Negative binomial and mixed Poisson regression.
<em>The Canadian Journal of Statistics</em>
<b>15</b>, 209â€“225.
</p>
<p>Miranda-Soberanis, V. F. and Yee, T. W. (2023).
Two-parameter link functions, with
applications to negative binomial, Weibull and
quantile regression.
<em>Computational Statistics</em>,
<b>38</b>, 1463â€“1485.
</p>
<p>Yee, T. W. (2014).
Reduced-rank vector generalized linear models with two linear
predictors.
<em>Computational Statistics and Data Analysis</em>,
<b>71</b>, 889â€“902.
</p>
<p>Yee, T. W. (2020).
The <span class="pkg">VGAM</span> package for negative binomial regression.
<em>Australian &amp; New Zealand Journal of Statistics</em>,
<b>62</b>, 116â€“131.
</p>


<h3>See Also</h3>

<p><code>quasipoisson</code>,
<code>gaitdnbinomial</code>,
<code>poissonff</code>,
<code>zinegbinomial</code>,
<code>negbinomial.size</code> (e.g., NB-G),
<code>nbcanlink</code> (NB-C),
<code>posnegbinomial</code>,
<code>genpoisson1</code>,
<code>genpoisson2</code>,
<code>genpoisson0</code>,
<code>inv.binomial</code>,
<code>NegBinomial</code>,
<code>rrvglm</code>,
<code>cao</code>,
<code>cqo</code>,
<code>CommonVGAMffArguments</code>,
<code>simulate.vlm</code>,
<code>ppoints</code>,
<code>margeff</code>.
</p>







<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# Example 1: apple tree data (Bliss and Fisher, 1953)
appletree &lt;- data.frame(y = 0:7, w = c(70, 38, 17, 10, 9, 3, 2, 1))
fit &lt;- vglm(y ~ 1, negbinomial(deviance = TRUE), data = appletree,
            weights = w, crit = "coef")  # Obtain the deviance
fit &lt;- vglm(y ~ 1, negbinomial(deviance = TRUE), data = appletree,
            weights = w, half.step = FALSE)  # Alternative method
summary(fit)
coef(fit, matrix = TRUE)
Coef(fit)  # For intercept-only models
deviance(fit)  # NB2 only; needs 'crit="coef"' &amp; 'deviance=T' above

# Example 2: simulated data with multiple responses
ndata &lt;- data.frame(x2 = runif(nn &lt;- 200))
ndata &lt;- transform(ndata, y1 = rnbinom(nn, exp(1), mu = exp(3+x2)),
                          y2 = rnbinom(nn, exp(0), mu = exp(2-x2)))
fit1 &lt;- vglm(cbind(y1, y2) ~ x2, negbinomial, ndata, trace = TRUE)
coef(fit1, matrix = TRUE)

# Example 3: large counts implies SFS is used
ndata &lt;- transform(ndata, y3 = rnbinom(nn, exp(1), mu = exp(10+x2)))
with(ndata, range(y3))  # Large counts
fit2 &lt;- vglm(y3 ~ x2, negbinomial, data = ndata, trace = TRUE)
coef(fit2, matrix = TRUE)
head(weights(fit2, type = "working"))  # Non-empty; SFS was used

# Example 4: a NB-1 to estimate a NB with Var(Y)=phi0*mu
nn &lt;- 200  # Number of observations
phi0 &lt;- 10  # Specify this; should be greater than unity
delta0 &lt;- 1 / (phi0 - 1)
mydata &lt;- data.frame(x2 = runif(nn), x3 = runif(nn))
mydata &lt;- transform(mydata, mu = exp(2 + 3 * x2 + 0 * x3))
mydata &lt;- transform(mydata, y3 = rnbinom(nn, delta0 * mu, mu = mu))
plot(y3 ~ x2, data = mydata, pch = "+", col = "blue",
     main = paste("Var(Y) = ", phi0, " * mu", sep = ""), las = 1)
nb1 &lt;- vglm(y3 ~ x2 + x3, negbinomial(parallel = TRUE, zero = NULL),
            data = mydata, trace = TRUE)
# Extracting out some quantities:
cnb1 &lt;- coef(nb1, matrix = TRUE)
mydiff &lt;- (cnb1["(Intercept)", "loglink(size)"] -
           cnb1["(Intercept)", "loglink(mu)"])
delta0.hat &lt;- exp(mydiff)
(phi.hat &lt;- 1 + 1 / delta0.hat)  # MLE of phi
summary(nb1)
# Obtain a 95 percent confidence interval for phi0:
myvec &lt;- rbind(-1, 1, 0, 0)
(se.mydiff &lt;- sqrt(t(myvec) %*%  vcov(nb1) %*%  myvec))
ci.mydiff &lt;- mydiff + c(-1.96, 1.96) * c(se.mydiff)
ci.delta0 &lt;- ci.exp.mydiff &lt;- exp(ci.mydiff)
(ci.phi0 &lt;- 1 + 1 / rev(ci.delta0))  # The 95
Confint.nb1(nb1)  # Quick way to get it
# cf. moment estimator:
summary(glm(y3 ~ x2 + x3, quasipoisson, mydata))$disper

## End(Not run)
</code></pre>


</div>